{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Rq6xsPJUvkXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "df = pd.read_csv('College Football_Upload.csv')\n",
        "df.drop(df.tail(173).index,\n",
        "        inplace = True)\n",
        "df_NP = df.drop(columns=df.columns[-7:], axis=1)\n",
        "df_22 = df_NP.dropna(axis=1, how='any') # drop columns with NA values\n",
        "\n",
        "df_16 = df_NP.drop(df.tail(6).index)\n",
        "df_Playoffs_16 = df.drop(df.tail(6).index)\n",
        "df_16.dropna(axis=1, how='any', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_16.isnull().sum().sum() #Check if there are NA values"
      ],
      "metadata": {
        "id": "C8PEra5HfdD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_22.isnull().sum().sum() #Check if there are NA values"
      ],
      "metadata": {
        "id": "FE8g56GGgd5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  for i in range(len(dataset.columns)):\n",
        "      cor2 = dataset.iloc[:,i].corr(target) #scipy.stats.spearmanr(x, y)[0] and scipy.stats.kendalltau(x, y)[0]\n",
        "      column_headers = list(dataset.columns.values)\n",
        "      if(abs(cor2) > threshold):\n",
        "        data.append(dataset.iloc[:,i]) #make list of columns that meet the threshold\n",
        "      i = i + 1\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred, y_full):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/np.mean(y_full))*100\n",
        "  Normalized_MAE = (MAE/np.mean(y_full))*100\n",
        "  Avg_Normalized_Score = (Normalized_RMSE + Normalized_MAE)/2\n",
        "  print(f'Avg. Normalized Score:{ Avg_Normalized_Score:.1f}%')\n",
        "  print(f'Normalized RMSE:{ Normalized_RMSE:.1f}%')\n",
        "  print(f'Normalized MAE:{ Normalized_MAE:.2f}%')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  return Avg_Normalized_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(X, y, choice, predict_df): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0) #Train/Test\n",
        "\n",
        "  if(choice==\"Ridge\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(RidgeCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(LassoCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    l1_ratio = [0, 0.3, 0.5, 0.7, 0.9, 1]\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, max_iter=100000))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "\n",
        "  #Calculate the predicted values:\n",
        "  y_train_pred = pipeline.predict(X_train)\n",
        "  print(y_train_pred)\n",
        "  print()\n",
        "\n",
        "  y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = pipeline.predict(predict_df)\n",
        "  print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FpU87Mgvvx0s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def GBR_model(X,y, t, l, n, predict_df):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "  reg = GradientBoostingRegressor(tol = t, learning_rate = l, n_estimators=n, random_state=0) #default: tol = 0.0001, learning rate - 0.1, 100, friedman_mse\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = reg.predict(predict_df)\n",
        "  print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "def BR_model(X,y, predict_df):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "  reg = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = reg.predict(predict_df)\n",
        "  print(predictions)"
      ],
      "metadata": {
        "id": "ppib1skHfgGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XfM5w0Ncvynu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#SGD Regressor:\n",
        "def SGD_model(X,y, t, ep, predict_df):\n",
        "\n",
        "  reg = make_pipeline(SGDRegressor(max_iter=1000, tol=t, epsilon = ep)) #tol = 0.001, epsilon=0.1\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  predictions = reg.predict(predict_df)\n",
        "  print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras Sequential Neural Net\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "def Keras_model(X,y,e, u, u2, u3, u4, u5):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(u, input_dim=X_train.shape[1], activation='relu')) # Hidden 1, 60\n",
        "  model.add(Dense(units=u2,activation='relu')) # Hidden 2, 30\n",
        "  model.add(Dense(units=u3,activation='relu'))\n",
        "  model.add(Dense(units=u4,activation='relu'))\n",
        "  model.add(Dense(units=u5,activation='relu'))\n",
        "  model.add(Dense(units=15,activation='relu')) #15\n",
        "  model.add(Dense(units=1)) #,activation='relu'\n",
        "  model.compile(loss='mean_squared_error', optimizer='nadam') #adam, nadam; adamax\n",
        "  m1 = model.fit(X_train, y_train, verbose=0, epochs=e, callbacks=[early_stop]); #callbacks=[early_stop]\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  model.save('/content/drive/MyDrive/Models/myModel7', save_format=\"h5\")"
      ],
      "metadata": {
        "id": "Zzt-o4jPgtdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def DTR_model(X,y,leafs):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "  # We introduce regularization by increasing the value of min_samples_leaf\n",
        "  tree_reg_regularized = DecisionTreeRegressor(random_state=42, min_samples_leaf=leafs)\n",
        "  tree_reg_regularized.fit(X_train, y_train)\n",
        "  y_train_pred = tree_reg_regularized.predict(X_train) #_regularized\n",
        "  print(y_train_pred)\n",
        "\n",
        "  y_test_pred = tree_reg_regularized.predict(X_test) #_regularized\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "sOg9t7czmLFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "def SVM_model(X,y,ep):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "  svm_reg = LinearSVR(epsilon=ep, random_state=42) #default: epsilon = 0 tol=0.0001, C=1.0\n",
        "  svm_reg.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = svm_reg.predict(X_train)\n",
        "  print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = svm_reg.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "luksQpOa9mx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def SVM_rbf_model(X,y, choice, ep, C_value):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "  if(choice==\"rbf\"):\n",
        "    model = SVR(kernel=\"rbf\", C=C_value, gamma=0.1, epsilon=ep) #0.1 default ep; 100 default C, 0.1 default gamma\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"poly\"):\n",
        "    model = SVR(kernel=\"poly\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "1ms2SSQ_3ncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In case I wanted to add these values to data frame of inputs\n",
        "scaler = StandardScaler()\n",
        "S = df_22[\"Avg/SOS_ScoringD\"].values\n",
        "S3 = S.reshape(-1, 1)\n",
        "data_scaledOPPG = scaler.fit_transform(S3)"
      ],
      "metadata": {
        "id": "ilrGRE6eLa_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding"
      ],
      "metadata": {
        "id": "UhxkZpeNTZA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scale:\n",
        "scaler = StandardScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(df_16), columns = df_16.columns)\n",
        "\n",
        "#Get Predictors:\n",
        "data_correlated = correlation(data_scaled, .4, df_Playoffs_16[\"Points/GM_Playoffs\"]) #.115 lowest for All csv\n",
        "data_correlated_df = pd.DataFrame(data_correlated)\n",
        "data_correlated_df2 = data_correlated_df.transpose() #Correlated inputs\n",
        "#data_correlated_df2['Avg/SOS_ScoringD'] = data_scaledOPPG\n",
        "#data_correlated_df2['Gms_Playoffs'] = data_scaledGMs\n",
        "pca=PCA(n_components = 4)\n",
        "data_PCA = pca.fit_transform(data_correlated_df2) #PCA inputs\n",
        "#X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=3, random_state=42) #n_components=2 is default, neighbors 5 is default\n",
        "X_unrolled = lle.fit_transform(data_correlated_df2)\n",
        "dfLLE = pd.DataFrame(X_unrolled)\n",
        "print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "print(\"Mean:\", pca.mean_)\n",
        "X = data_PCA #try data_correlated_df2, data_PCA, or dfLLE\n",
        "y = df_Playoffs_16[\"Points/GM_Playoffs\"]"
      ],
      "metadata": {
        "id": "Z0RXs3FQMoHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_correlated)"
      ],
      "metadata": {
        "id": "zCYcvQ1wa8Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in input data from this year's 4 CFP teams to predict oPPG\n",
        "predictors_defense_df = pd.read_csv('Predictors_Defense_2023.csv')\n",
        "predictors_defense_df.drop(predictors_defense_df.tail(2).index,\n",
        "        inplace = True)\n",
        "predictors_defense = lle.fit_transform(predictors_defense_df)"
      ],
      "metadata": {
        "id": "3IEqiiHDxZRR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict oPPG\n",
        "#oPPG Blender: #22.1%; 4.4% difference\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "oPPG_model = tensorflow.keras.models.load_model('/content/drive/MyDrive/Models/myModel1') #23.6%\n",
        "preds = oPPG_model.predict(predictors_defense)\n",
        "oPPG_model2 = tensorflow.keras.models.load_model('/content/drive/MyDrive/Models/myModel4') #24.8%\n",
        "preds2 = oPPG_model2.predict(predictors_defense)\n",
        "oPPG_model3 = tensorflow.keras.models.load_model('/content/drive/MyDrive/Models/myModel6') #22.8%\n",
        "preds3 = oPPG_model3.predict(predictors_defense)\n",
        "oPPG_model4 = tensorflow.keras.models.load_model('/content/drive/MyDrive/Models/myModel2') #22.8%, -2.4%, model2, 400, 210, 200, 190, 100, 36\n",
        "preds4 = oPPG_model4.predict(predictors_defense)\n",
        "final_test_preds = (preds + preds2 + preds3 + preds4)/4\n",
        "print(final_test_preds)\n",
        "#Scores(y_train, final_test_preds, y)"
      ],
      "metadata": {
        "id": "PPMueSsP3QzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6708de-067f-4860-b259-85f46bf9e5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 254ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 446ms/step\n",
            "1/1 [==============================] - 0s 449ms/step\n",
            "[[36.35722 ]\n",
            " [39.6763  ]\n",
            " [49.715473]\n",
            " [36.920113]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in input data from this year's 4 CFP teams to predict PPG\n",
        "predictors_df = pd.read_csv('Predictors_2023.csv')\n",
        "predictors_df.drop(predictors_df.tail(10).index,\n",
        "        inplace = True)\n",
        "predictors = pca.fit_transform(predictors_df)"
      ],
      "metadata": {
        "id": "pXrUnJ6UlQnt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict PPG:\n",
        "GBR_model(X,y, .0001, 0.008, 100, predictors)"
      ],
      "metadata": {
        "id": "vv863TWzzc9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a4b734-b61e-4dc4-eb91-5d5365c854f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg. Normalized Score:11.4%\n",
            "Normalized RMSE:12.2%\n",
            "Normalized MAE:10.46%\n",
            "\n",
            "Test predictions:\n",
            "[30.57090341 39.04558959 34.01120864 33.93328673 39.04558959]\n",
            "\n",
            "Avg. Normalized Score:11.5%\n",
            "Normalized RMSE:13.1%\n",
            "Normalized MAE:9.94%\n",
            "Difference of avg scores:0.160%\n",
            "\n",
            "[40.88892146 29.6119849  29.6119849  41.02980684]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30.57090341, 39.04558959, 34.01120864, 33.93328673, 39.04558959])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PPG Model Blender:\n",
        "y_test1 = RLE_Model(X, y, \"Lasso\", predictors) #.4, data_PCA\n",
        "y_test2 = GBR_model(X,y, .0001, 0.008, 100, predictors)\n",
        "y_test3 = SGD_model(X,y, 1e-3, 0.1, predictors) #.4, data_PCA\n",
        "final_test_preds = (y_test1 + y_test2 + y_test3)/3\n",
        "y_train, y_test = train_test_split(y, test_size=0.30, random_state=0)\n",
        "Scores(y_test, final_test_preds, y) #10.3%"
      ],
      "metadata": {
        "id": "aCx23pZ3vF9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model: (Points Scored)\n",
        "RLE_Model(X, y, \"Lasso\", predictors) #10.6% for Lasso, diff 4.2%, .4, data_PCA\n",
        "SGD_model(X,y, 1e-3, 0.1, predictors) #9.9% for data_PCA, diff 4.5%., .4, 1e-3, .1\n",
        "GBR_model(X,y, .0001, 0.1, 100, predictors) #11.5%, .1%, .4, data_PCA, .0001, .008, 100\n",
        "BR_model(X,y, predictors)\n",
        "Keras_model(X, y, 200, 120, 60, 30) #9.9% for points per play dfLLE, .45, 100, 40, 30\n",
        "DTR_model(X,y, 100) #12.2% for data PCA, 12% diff, 100, .5, other set\n",
        "SVM_model(X,y, 0) #0 default\n",
        "SVM_rbf_model(X, y, \"rbf\", .1, 100) #.1; 100 default, 16.2%, -.3% diff, .45, 5.5, 8, data_PCA,\n",
        "SVM_rbf_model(X, y, \"poly\", .1, 100) #.1; 100 default, 17.5%, .3% diff, .4, 6, 100, data_correlated_df2"
      ],
      "metadata": {
        "id": "7LSkfJVdFcDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model: (points allowed)\n",
        "RLE_Model(X, y, \"Ridge\", predictors_defense) #Ridge\n",
        "SGD_model(X,y, 1e-3, 0.1, predictors_defense) #0.1 default\n",
        "GBR_model(X,y, .0001, 0.1, 100, predictors_defense) #0.1 default\n",
        "BR_model(X,y, predictors_defense)\n",
        "Keras_model(X, y, 400, 210, 200, 190, 100, 36) #22.8%, -2.4%, model2, 400, 210, 200, 190, 100, 36\n",
        "DTR_model(X,y, 100) #100 default\n",
        "SVM_model(X,y, 0) #0 default\n",
        "SVM_rbf_model(X, y, \"rbf\", .1, 100) #.1; 100 default\n",
        "SVM_rbf_model(X, y, \"poly\", .1, 100) #.1; 100 default, 21% for per play, dfLLE, .12, 180"
      ],
      "metadata": {
        "id": "kq7ZsczOFdIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_correlated)"
      ],
      "metadata": {
        "id": "vW0Gnxhr7Fah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_correlated_df2.columns"
      ],
      "metadata": {
        "id": "jKS4ZL8DJw5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dfLLE)"
      ],
      "metadata": {
        "id": "pQ6OCieYhbyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_PCA)"
      ],
      "metadata": {
        "id": "p8SgF713kfIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for collinearity\n",
        "import seaborn as sns\n",
        "#sns.pairplot(data_correlated_df2)\n",
        "\n",
        "corr = data_correlated_df2.corr()\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "TyALuH9TPDdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Pearson's correlation between 2 variables\n",
        "df_16.iloc[:,-3].corr(df_Playoffs_16.iloc[:,-2])"
      ],
      "metadata": {
        "id": "mmHqjfoXJx3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}